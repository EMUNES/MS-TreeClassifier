{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('tlc': conda)",
   "display_name": "Python 3.8.5 64-bit ('tlc': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7c33d18c879b8c16ea1940d8472bc6cf82c89fa81050fa6bd15d10c212b5b695"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pdb\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models\n",
    "from efficientnet_pytorch import EfficientNet \n",
    "\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensor, ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "source": [
    "### global configurations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Those are the core configurations of this notebook. \n",
    "Read this carefully: \n",
    "\n",
    "- paths do not need to change unless you changed folder structure of this project \n",
    "- if you add more data (especially add a folder full of images under ./data/train), you should only need to alter *label_encoding*\n",
    "- each time you want to train and save a new model, please change *model_name* so you won't rewrite other's result\n",
    "- *epoch* stands for the total epochs of training. 8-15 should be sufficient for this task. You can add up epochs but you'd better do not want to surpass 20\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./data/')\n",
    "path_train = path/'train'\n",
    "path_test = path/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size, bs = 224, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9,\n",
       " {'Chinar': 0,\n",
       "  'Gauva': 1,\n",
       "  'Jamun': 2,\n",
       "  'mahogany': 3,\n",
       "  'cedar': 4,\n",
       "  'Elaeocarpus sylvestris': 5,\n",
       "  'balsam poplar': 6,\n",
       "  'cottonwood': 7,\n",
       "  'honey locust': 8})"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# encode tree types to integer labels range from 0\n",
    "# the string must be the same as the folder name the ./data/train/xxx\n",
    "# which stands for the tree type that images belong to \n",
    "label_encoding = (\n",
    "    (\"Chinar\", 0), \n",
    "    (\"Gauva\", 1),\n",
    "    (\"Jamun\", 2),\n",
    "    (\"mahogany\", 3),\n",
    "    (\"cedar\", 4),\n",
    "    (\"Elaeocarpus sylvestris\", 5),\n",
    "    (\"balsam poplar\", 6),\n",
    "    (\"cottonwood\", 7),\n",
    "    (\"honey locust\", 8),\n",
    ")\n",
    "num_labels = len(label_encoding)\n",
    "label_encoding = dict(label_encoding)\n",
    "num_labels, label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name that will be saved after all epochs\n",
    "model_name = 'c10-e12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total epochs\n",
    "epoch = 12 "
   ]
  },
  {
   "source": [
    "## helper functions to load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts all your images under different folders, for example:\n",
    "# - train \n",
    "# - - tree type 1\n",
    "# - - - all tree type 1 images...\n",
    "# - - tree type 2\n",
    "# - - - ...\n",
    "# - - tree type 3\n",
    "# - - - ...\n",
    "def list_all_train_files(path: Path):\n",
    "    '''Return all image file paths in a list\n",
    "\n",
    "    Returns:\n",
    "        files: a list contains all image file paths\n",
    "\n",
    "    Args:\n",
    "        path: the path that holds all the images\n",
    "    '''\n",
    "    files = []\n",
    "    for o in path.iterdir():\n",
    "        files.extend([f for f in o.iterdir()])\n",
    "    return files\n",
    "\n",
    "# for all files in the same folder\n",
    "# def list_all_train_files(path:Path):\n",
    "#     return [f for f in path.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([WindowsPath('data/train/balsam poplar/image9.jpeg'),\n",
       "  WindowsPath('data/train/balsam poplar/images151.jpg'),\n",
       "  WindowsPath('data/train/honey locust/images175.jpg'),\n",
       "  WindowsPath('data/train/Chinar/0022_0014.JPG'),\n",
       "  WindowsPath('data/train/Chinar/0011_0015.JPG')],\n",
       " 3393)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_fnames = list_all_train_files(path_train)\n",
    "random.shuffle(train_fnames)\n",
    "len_fnames = len(train_fnames)\n",
    "train_fnames[:5], len_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices = len_fnames//bs\n",
    "train_slices = -slices//5\n",
    "\n",
    "train_fnames = train_fnames[:train_slices*bs]\n",
    "valid_fnames = train_fnames[train_slices*bs:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([], 0)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "test_fnames = list_all_train_files(path_test)\n",
    "test_fnames[:5], len(test_fnames)"
   ]
  },
  {
   "source": [
    "## Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, f_paths: list, transforms=None, is_test=False):\n",
    "        self.f_paths = f_paths    \n",
    "        self.transforms = transforms\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1.get image file\n",
    "        img_path = self.f_paths[index]\n",
    "        image = np.array(Image.open(img_path), dtype=np.float32)\n",
    "\n",
    "        # transform?\n",
    "        if self.transforms:\n",
    "            image = self.transforms(**{'image': image})['image']\n",
    "\n",
    "        # test?\n",
    "        if self.is_test:\n",
    "            return image\n",
    "\n",
    "        # 2.get the corresponding label to this image\n",
    "        tree_type = str(img_path).split('\\\\')[-2]\n",
    "        label = label_encoding[tree_type]\n",
    "        target = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.f_paths)"
   ]
  },
  {
   "source": [
    "## model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeEfficientNet(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet-b3', pool_type=F.adaptive_avg_pool2d):\n",
    "        super(TreeEfficientNet, self).__init__()\n",
    "        self.pool_type = pool_type\n",
    "        self.backbone = EfficientNet.from_pretrained(model_name)\n",
    "        \n",
    "        image_in_features = getattr(self.backbone, '_fc').in_features\n",
    "        self.efn_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(image_in_features, 512),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(256, 128),\n",
    "        )\n",
    "        self.classifer = nn.Linear(128, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_features = self.pool_type(self.backbone.extract_features(x), 1)\n",
    "        cnn_features = cnn_features.view(x.size(0), -1)\n",
    "        cnn_features = self.efn_head(cnn_features)\n",
    "\n",
    "        return self.classifer(cnn_features)"
   ]
  },
  {
   "source": [
    "## Focal Loss"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, preds, truth):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pt = criterion(preds, truth.to(dtype=torch.long))\n",
    "        log_pt = torch.log(pt)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * log_pt\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "source": [
    "## helper functions for forward&backward propagation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device\n",
    "\n",
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just for cleaning the possible None values\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [(data, target) for (data, target) in batch if data is not None]\n",
    "    return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shit ton of augmentations using albumentations\n",
    "\n",
    "def get_augmentations(p=0.5, img_size=image_size):\n",
    "    # give pretrained image_net stats\n",
    "    imagenet_stats = {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}\n",
    "    \n",
    "    # this is for training\n",
    "    train_tfms = A.Compose([\n",
    "        # simple cutout regularization\n",
    "        A.Cutout(p=p),\n",
    "        # rotation\n",
    "        #A.RandomRotate90(p=p),\n",
    "        #A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=p),\n",
    "        # flip\n",
    "        A.Flip(p=p),\n",
    "        # one of color augmentation\n",
    "        A.OneOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                      contrast_limit=0.2,),\n",
    "            A.HueSaturationValue(\n",
    "                hue_shift_limit=20,\n",
    "                sat_shift_limit=50,\n",
    "                val_shift_limit=50)\n",
    "        ], p=p),\n",
    "        # one of noise augmentation\n",
    "        A.OneOf([\n",
    "            A.IAAAdditiveGaussianNoise(),\n",
    "            A.GaussNoise()\n",
    "        ], p=p),\n",
    "        # one of blurring augmenation\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(p=0.2),\n",
    "            A.MedianBlur(blur_limit=3, p=0.1),\n",
    "            A.Blur(blur_limit=3, p=0.1),\n",
    "        ], p=p),\n",
    "        # one of distortion\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(p=0.3),\n",
    "            A.GridDistortion(p=0.1),\n",
    "            A.IAAPiecewiseAffine(p=0.3),\n",
    "        ], p=p),\n",
    "        A.Resize(img_size, img_size, always_apply=True),\n",
    "        # must do: to tensor\n",
    "        ToTensor(normalize=imagenet_stats),\n",
    "    ])\n",
    "    \n",
    "    # this is for TTA\n",
    "    test_tfms = A.Compose([\n",
    "        A.RandomRotate90(p=p),\n",
    "            A.Flip(p=p),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                           contrast_limit=0.2,\n",
    "                                           ),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=20,\n",
    "                    sat_shift_limit=50,\n",
    "                    val_shift_limit=50)\n",
    "            ], p=p),\n",
    "            A.OneOf([\n",
    "                A.IAAAdditiveGaussianNoise(),\n",
    "                A.GaussNoise(),\n",
    "            ], p=p),\n",
    "        ToTensor(normalize=imagenet_stats)\n",
    "        ])\n",
    "\n",
    "    valid_tfms = A.Compose([\n",
    "        ToTensor(normalize=imagenet_stats)\n",
    "    ])\n",
    "\n",
    "    return train_tfms, valid_tfms, test_tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_tfms, valid_tfms):\n",
    "    train_ds = TreeDataset(train_fnames, train_tfms)\n",
    "    #valid_ds = TreeDataset(train_fnames, valid_tfms)\n",
    "    valid_ds = TreeDataset(valid_fnames, valid_tfms)\n",
    "    train_dl = DataLoader(dataset=train_ds, batch_size=bs, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "    valid_dl = DataLoader(dataset=valid_ds, batch_size=bs, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "source": [
    "## Set up our model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name='efficientnet-b3', lr=1e-5, wd=0.01, freeze_backbone=False, opt_fn=torch.optim.AdamW, device=None):\n",
    "    # 1. get device\n",
    "    device = device if device else get_device()\n",
    "    # 2.get our model\n",
    "    pool_type = F.adaptive_avg_pool2d\n",
    "    model = TreeEfficientNet(model_name=model_name, pool_type=pool_type)\n",
    "    if freeze_backbone:\n",
    "        for parameter in model.backbone.parameters():\n",
    "            parameter.requires_grad = False\n",
    "    # 3. get our optimizer for back propagation - AdamW tends to work better\n",
    "    optimizer = opt_fn(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    # 4. move our model to device\n",
    "    model.to(device)\n",
    "\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(xb, yb, model, loss_fn, opt, device, scheduler):\n",
    "    # forward\n",
    "    xb, yb = xb.to(device), yb.reshape(-1).to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_fn(out, yb)\n",
    "\n",
    "    # backward\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(xb, yb, model, loss_fn, device):\n",
    "    xb, yb = xb.to(device), yb.reshape(-1).to(device)\n",
    "    out = model(xb)\n",
    "    loss = loss_fn(out, yb)\n",
    "    \n",
    "    out = torch.sigmoid(out)\n",
    "    \n",
    "    return loss.item(), out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap up to a fit one cycle funcition\n",
    "def fit(epochs, train_dl, valid_dl, model, loss_fn, opt, device=None):\n",
    "    # set up device for data\n",
    "    device = device if device else get_device()\n",
    "    # set up scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, len(train_dl)*epochs)\n",
    "    val_accuracy_scores = []\n",
    "    \n",
    "    # creating a progress bar\n",
    "    mb = master_bar(range(epochs))\n",
    "    mb.write(['epochs', 'train_loss', 'valid_loss', 'accuracy'], table=True)\n",
    "    \n",
    "    # iterate 10 epochs\n",
    "    for epoch in mb:\n",
    "        trn_loss, val_loss = 0., 0.\n",
    "        val_preds = np.zeros((len(valid_dl.dataset), num_labels))\n",
    "        val_targets = np.zeros((len(valid_dl.dataset), 1))\n",
    "        val_scores = []\n",
    "        \n",
    "        # training mode\n",
    "        model.train()\n",
    "        \n",
    "        # for every batch, we step and collect training loss\n",
    "        for xb, yb in progress_bar(train_dl, parent=mb):\n",
    "            trn_loss += training_step(xb, yb, model=model, loss_fn=loss_fn, opt=opt, device=device, scheduler=scheduler)\n",
    "        trn_loss /= mb.child.total # 10\n",
    "        \n",
    "        # validation mode\n",
    "        # now we need valid_loss and val_score from the validatin steps (witout gradients of course)\n",
    "        with torch.no_grad():\n",
    "            for i, (xb, yb) in enumerate(progress_bar(valid_dl, parent=mb)):\n",
    "                loss, out = validation_step(xb, yb, model=model, loss_fn=loss_fn, device=device)\n",
    "                val_loss += loss\n",
    "                bs = xb.shape[0]\n",
    "                val_preds[i*bs: i*bs+bs] = out.cpu().numpy()\n",
    "                val_targets[i*bs: i*bs+bs]= yb.cpu().numpy()\n",
    "                \n",
    "        preds = np.argmax(softmax(val_preds, axis=1), axis=1)\n",
    "        true = val_targets.reshape(-1)\n",
    "        accuracy = accuracy_score(true, preds)\n",
    "        val_accuracy_scores.append(accuracy)\n",
    "        mb.write([epoch, f'{trn_loss:.6f}', f'{val_loss:.6f}', f'{accuracy:.6f}'], table=True)\n",
    "\n",
    "    return model, val_accuracy_scores"
   ]
  },
  {
   "source": [
    "## start training validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epochs</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.264775</td>\n      <td>5.617484</td>\n      <td>0.225852</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.247756</td>\n      <td>5.081916</td>\n      <td>0.306818</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.218219</td>\n      <td>4.200132</td>\n      <td>0.339489</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.178175</td>\n      <td>3.254919</td>\n      <td>0.338068</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.151582</td>\n      <td>2.585804</td>\n      <td>0.339489</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.137812</td>\n      <td>2.254083</td>\n      <td>0.400568</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.133445</td>\n      <td>2.019162</td>\n      <td>0.404830</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.123297</td>\n      <td>1.817188</td>\n      <td>0.419034</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.121703</td>\n      <td>1.668018</td>\n      <td>0.428977</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.120398</td>\n      <td>1.766898</td>\n      <td>0.392045</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.113693</td>\n      <td>1.617629</td>\n      <td>0.424716</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.115944</td>\n      <td>1.611821</td>\n      <td>0.443182</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "train_tfms, valid_tfms, test_tfms = get_augmentations()\n",
    "# get train and validation dataloader\n",
    "train_dl, valid_dl = get_data(train_tfms=train_tfms, valid_tfms=valid_tfms)\n",
    "# get our loss func\n",
    "loss_fn = FocalLoss(alpha=0.25, gamma=2)\n",
    "\n",
    "model, opt = get_model(model_name='efficientnet-b3', lr=1e-5, wd=1e-2)\n",
    "\n",
    "model, accuracy_score = fit(epoch, train_dl, valid_dl, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training finished, TOTAL epochs: 12\nSaving model as :c10-e12\nYou can check your model in ./models\n"
     ]
    }
   ],
   "source": [
    "print(f'Training finished, TOTAL epochs: {epoch}\\nSaving model as :{model_name}')\n",
    "torch.save(model.state_dict(), f'./models/{model_name}.pth')\n",
    "print('You can check your model in ./models')"
   ]
  }
 ]
}